{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will provide simple functions to clean translation memory file that contains bad encoding and generate train/test and tune datasets from the sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get, post, delete\n",
    "import json\n",
    "from translate.storage.tmx import tmxfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean bad XML encding from translation memory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we replace bad XML encoding used for example by previous version of Trados\n",
    "\n",
    "inputfile = 'ENG-SPA_Reference.tmx'\n",
    "outputfile = 'ENG-SPA_Reference-edited.tmx'\n",
    "\n",
    "from re import search\n",
    "\n",
    "#  Bad encoding to remove\n",
    "substring = \"#x1E;\"\n",
    "\n",
    "with open(outputfile, \"ab\") as output_file:\n",
    "    with open(inputfile, 'r', encoding='utf-8') as input_file:\n",
    "        line = input_file.readline()\n",
    "        while line:\n",
    "            new_line = line\n",
    "            found =  search(substring, line)\n",
    "            if not found is None:\n",
    "                new_line = line[:int(found.span(0)[0])] + line[int(found.span(0)[1]):]\n",
    "            \n",
    "            \n",
    "            amp = search('&', new_line)\n",
    "            if not amp is None:\n",
    "                if ('<SEG>' in new_line) or ('<seg>' in new_line) or ('</SEG>' in new_line) or ('</seg>' in new_line):\n",
    "                    new_line = new_line.replace('&',' and ')\n",
    "            \n",
    "            output_file.write(new_line.encode('utf-8'))\n",
    "            line = input_file.readline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we load a cleaned translation memory file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ENG-FRE_Reference-edited.tmx\", 'rb') as en_es:\n",
    "    tmx_file = tmxfile(en_es, 'en-GB', 'fr-FR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will chunk the files and create the train/test/tune datasets for Trados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This approach simply samples the first n suitable records for the test and train sets. All datasets are \n",
    "# exclusive in that records only exist in a single dataset i.e. train/test/tune\n",
    "# Ensure train/test/tune directories exist where we will write to\n",
    "\n",
    "\n",
    "import tqdm as tqdm\n",
    "\n",
    "max_unit_count = 99500    #  As we have a size limit of file size, let's set an upper limit\n",
    "max_test_count = 2450     # We can only have 2500 sentence pairs in the test set\n",
    "max_tune_count = 2450     # We can only have 2500 sentence pairs in the tune set\n",
    "min_tune_token_count = 7  # We ideally want tune sentences to be minimum this amount\n",
    "max_tune_token_count = 10 # We ideally want tune sentences to be minimum this amount\n",
    "train_unit_count = 0\n",
    "test_unit_count = 0\n",
    "tune_unit_count = 0\n",
    "last_unit_processed = 0\n",
    "file_name_suffix = '_.tmx'\n",
    "current_batch_num = 0\n",
    "\n",
    "master_tmx = 'ENG-SPA_Reference-edited.tmx'\n",
    "\n",
    "# Change this header for the file processed, we add this to retain metadata of the original translation memory\n",
    "# file\n",
    "eng_spa = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<tmx version=\"1.4\">\n",
    "  <header creationtool=\"SDL Language Platform\" creationtoolversion=\"8.0\" o-tmf=\"SDL TM8 Format\" datatype=\"xml\" segtype=\"sentence\" adminlang=\"en-US\" srclang=\"en-US\" creationdate=\"20150115T221603Z\" creationid=\"nashm\">\n",
    "    <prop type=\"x-Job Num.:MultipleString\"></prop>\n",
    "    <prop type=\"x-Symbol:MultipleString\"></prop>\n",
    "    <prop type=\"x-Title:MultipleString\"></prop>\n",
    "    <prop type=\"x-Recognizers\">RecognizeDates, RecognizeTimes, RecognizeNumbers, RecognizeMeasurements</prop>\n",
    "    <prop type=\"x-TMName\">ENG-SPA</prop>\n",
    "  </header>\n",
    "  <body>\"\"\"\n",
    "\n",
    "# Change this header for the file processed, we add this to retain metadata of the original translation memory\n",
    "# file\n",
    "eng_spa_ref = \"\"\"<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<tmx version=\"1.4\">\n",
    "  <header creationtool=\"SDL Language Platform\" creationtoolversion=\"8.0\" o-tmf=\"SDL TM8 Format\" datatype=\"xml\" segtype=\"sentence\" adminlang=\"en-US\" srclang=\"en-US\" creationdate=\"20150115T221744Z\" creationid=\"nashm\">\n",
    "    <prop type=\"x-Job Num.:MultipleString\"></prop>\n",
    "    <prop type=\"x-Symbol:MultipleString\"></prop>\n",
    "    <prop type=\"x-Title:MultipleString\"></prop>\n",
    "    <prop type=\"x-Recognizers\">RecognizeDates, RecognizeTimes, RecognizeNumbers, RecognizeMeasurements</prop>\n",
    "    <prop type=\"x-TMName\">ENG-SPA-Reference</prop>\n",
    "  </header>\n",
    "  <body>\"\"\"\n",
    "\n",
    "header = eng_spa_ref\n",
    "\n",
    "# The footer is a simple XML closing tag\n",
    "footer = r\"\"\"  </body>\n",
    "</tmx>\"\"\"\n",
    "\n",
    "# We can optionally generate a tuning file if set to True\n",
    "generate_tuning_file = False\n",
    "# We can optionally generate a test file if set to True\n",
    "generate_test_file = False\n",
    "f_train = None\n",
    "f_test = open('test/' + master_tmx[:-4] + '_test.tmx' ,\"ab\")\n",
    "f_test.write(header.encode('utf-8'))\n",
    "\n",
    "\n",
    "with open(master_tmx, 'rb') as en_es:\n",
    "    tmx_file = tmxfile(en_es, 'en-GB', 'en-ES')  # The translation library does not filter on these values\n",
    "                                                 # so no need to change\n",
    "    \n",
    "    \n",
    "print(f\"Loaded {master_tmx}\")\n",
    "\n",
    "\n",
    "if generate_tuning_file:\n",
    "    f_tune = open('tune/' + master_tmx[:-4] + '_tune.tmx' ,\"ab\")\n",
    "    f_tune.write(header.encode('utf-8'))\n",
    "\n",
    "for i, unit in enumerate(tmx_file.unit_iter()):\n",
    "    \n",
    "\n",
    "    processed = False\n",
    "    \n",
    "    # Prepare our file name\n",
    "    if i % max_unit_count == 0:\n",
    "        print(f\"Running batch {i} of {len(tmx_file.units)}\")\n",
    "        current_batch_num = int(i/max_unit_count)\n",
    "        \n",
    "        if not f_train is None:\n",
    "            f_train.write(footer.encode('utf-8'))\n",
    "            f_train.close()\n",
    "            print(f\"Wrote {'train/' + master_tmx[:-4] + str(current_batch_num) + '_train.tmx'}\")\n",
    "            \n",
    "        f_train = open('train/' + master_tmx[:-4] + str(current_batch_num) + '_train.tmx' ,\"ab\")\n",
    "        f_train.write(header.encode('utf-8'))\n",
    "    \n",
    "    if generate_tuning_file:\n",
    "        # Get the word count in the source language - we will take the first 2500 that meet our token criteria\n",
    "        lst_source_text = unit.getid().split()\n",
    "\n",
    "        if tune_unit_count < max_tune_count:\n",
    "            if (len(lst_source_text) <= max_tune_token_count) and (len(lst_source_text) >= min_tune_token_count):\n",
    "                f_tune.write(str(unit).replace(\"en-GB\",\"en-US\").encode('utf-8'))\n",
    "                tune_unit_count += 1\n",
    "                processed = True\n",
    "                \n",
    "    if not processed:\n",
    "        if generate_test_file:\n",
    "            if test_unit_count < max_test_count:\n",
    "                    # Randomly sample test files every n files\n",
    "                    if i % 100 == 0:\n",
    "                        f_test.write(str(unit).replace(\"en-GB\",\"en-US\").encode('utf-8'))\n",
    "                        test_unit_count += 1\n",
    "                        processed = True\n",
    "                \n",
    "\n",
    "    if not processed:\n",
    "        # Sample the first 2500 for test\n",
    "        f_train.write(str(unit).replace(\"en-GB\",\"en-US\").encode('utf-8'))\n",
    "        train_unit_count += 1\n",
    "\n",
    "\n",
    "if generate_tuning_file:                \n",
    "    f_tune.write(footer.encode('utf-8'))\n",
    "    f_tune.close()\n",
    "\n",
    "if generate_test_file:\n",
    "    f_test.write(footer.encode('utf-8'))\n",
    "    f_test.close()\n",
    "\n",
    "f_train.write(footer.encode('utf-8'))\n",
    "f_train.close()\n",
    "\n",
    "\n",
    "print(f\"Ran {current_batch_num} batches. Generated {train_unit_count*current_batch_num} train records\")\n",
    "print(f\" Generated {test_unit_count} test records\")\n",
    "print(f\" Generated {tune_unit_count} tune records\")            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translation",
   "language": "python",
   "name": "translation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}